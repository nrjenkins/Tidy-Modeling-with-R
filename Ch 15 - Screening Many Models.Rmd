---
title: 'Chapter 15: Screening Many Models'
author: "Nick Jenkins"
date: '2022-04-18'
output: html_document
---

# Modeling Concrete Mixture Strength

First, we define the data splitting and rescampling schemes:

```{r}
library(tidymodels)
tidymodels_prefer()

data("concrete", package = "modeldata")
glimpse(concrete)

# find the mean compressive strength per concrete mixture
concrete <- 
  concrete %>% 
  group_by(across(-compressive_strength)) %>% 
  summarize(compressive_strength = mean(compressive_strength),
            .groups = "drop")
glimpse(concrete)
```

Now we split the data and resample it:

```{r}
set.seed(1501)
concrete_split <- initial_split(concrete, strata = compressive_strength)
concrete_train <- training(concrete_split)
concrete_test <- testing(concrete_split)

set.seed(1502)
concrete_folds <- 
  vfold_cv(concrete_train, strata = compressive_strength, repeats = 5)
```

We'll create two recipes:

```{r}
normalized_recipe <- 
  recipe(compressive_strength ~ ., data = concrete_train) %>% 
  step_normalize(all_predictors())

poly_recipe <- 
  normalized_recipe %>% 
  step_poly(all_predictors()) %>% 
  step_interact(~ all_predictors():all_predictors())
```

And now we make model specifications:

```{r}
library(rules)
library(baguette)

linear_reg_model <- 
  linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")

nnet_model <- 
  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% 
  set_engine("nnet", MaxMWts = 2600) %>% 
  set_mode("regression")

nnet_param <- 
  nnet_model %>% 
  extract_parameter_set_dials() %>% 
  update(hidden_units = hidden_units(c(1, 27)))

mars_model <- 
  mars(prod_degree = tune()) %>% 
  set_engine("earth") %>% 
  set_mode("regression")

svm_r_model <- 
  svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("regression")

svm_p_model <- 
  svm_poly(cost = tune(), degree = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("regression")

knn_model <- 
  nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %>% 
  set_engine("kknn") %>% 
  set_mode("regression")

cart_model <- 
  decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")

bag_cart_spec <- 
  bag_tree() %>% 
  set_engine("rpart", times = 50L) %>% 
  set_mode("regression")

rf_model <- 
  rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

xgb_model <- 
  boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(),
             min_n = tune(), sample_size = tune(), trees = tune()) %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

cubist_model <- 
  cubist_rules(committees = tune(), neighbors = tune()) %>% 
  set_engine("Cubist")
```

We can use a workflow set to match the models to their recipes, tune them, and evaluate their performance. 

# Creating the Workflow Set

Let's combine the recipe that only standardizes the predictors to the nonlinear models that require the predictors to be in the same units:

```{r}
normalized <- 
  workflow_set(preproc = list(normalized = normalized_recipe),
               models = list(svm_radial = svm_r_model, 
                             svm_poly = svm_p_model,
                             knn = knn_model,
                             neural_network = nnet_model))
normalized
```

Individual workflows can be extracted:

```{r}
normalized %>% extract_workflow(id = "normalized_knn")
```

For the other nonlinear models, we'll use **dplyr** selectors to create a workflow set. 

```{r}
model_vars <- 
  workflow_variables(outcomes = compressive_strength,
                     predictors = everything())

no_pre_proc <- 
  workflow_set(preproc = list(simple = model_vars),
               models = list(mars = mars_model,
                             cart = cart_model,
                             cart_bagged = bag_cart_spec,
                             rf = rf_model,
                             boosting = xgb_model,
                             cubist = cubist_model))

no_pre_proc
```

Finally, the set that uses nonlinear terms and interactions:

```{r}
with_features <- 
  workflow_set(preproc = list(full_qud = poly_recipe),
               models = list(linear_reg = linear_reg_model,
                             knn = knn_model))
```

These objects are tibbles with the extra class of `workflow_set()`. Row binding does not affect the state of the sets and the result is itself a workflow set:

```{r}
all_workflows <- 
  bind_rows(no_pre_proc, normalized, with_features) %>% 
  # make the workflow ID's a little more simple:
  mutate(wflow_id = gsub("(simple_)|(normalized_)", "", wflow_id))
all_workflows
```

# Tuning and Evaluating the Models

We will apply grid search to each workflow using up to 25 different parameter candidates. 

```{r}
library(doMC)
registerDoSEQ()

grid_ctrl <- 
  control_grid(save_pred = TRUE,
               parallel_over = "everything",
               save_workflow = TRUE)

grid_results <- 
  all_workflows %>% 
  workflow_map(seed = 1503,
               resamples = concrete_folds,
               grid = 25, 
               control = grid_ctrl)
```

There are some convenience functions for examining results such as `grid_results`. The `rank_results()` function will order the models by some performance metric. By default, it uses the first metric in the metric set. Let's `filter()` to only look are RMSE:

```{r}
grid_results %>% 
  rank_results() %>% 
  filter(.metric == "rmse") %>% 
  select(model, .config, rmse = mean, rank)
```

The `select_best()` option can also be used to rank models. The `autoplot()` method plots the rankings:

```{r}
autoplot(grid_results,
         rank_metric = "rmse", # how to order models
         metric = "rmse", # which metric to visualize
         select_best = TRUE) + # one point per workflow
  geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1)
```

# Efficiently Screening Models

One effective method for screening a larget set of model efficiently is to use the racing approach. 

```{r}
library(finetune)

race_ctrl <- 
  control_race(save_pred = TRUE,
               parallel_over = "everything",
               save_workflow = TRUE)

race_results <- 
  all_workflows %>% 
  workflow_map("tune_race_anova",
               seed = 1503,
               resamples = concrete_folds,
               grid = 25, 
               control = race_ctrl)
```

The race approach is much faster. 

```{r}
autoplot(race_results,
         rank_metric = "rmse", # how to order models
         metric = "rmse", # which metric to visualize
         select_best = TRUE) + # one point per workflow
  geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1)
```

# Finalizing a Model

To choose a final model and fit it on the entire training set, you pick a workflow to finalize. 

```{r}
best_results <- 
  race_results %>% 
  extract_workflow_set_result("boosting") %>% 
  select_best(metric = "rmse")
best_results

boosting_test_results <- 
  race_results %>% 
  extract_workflow("boosting") %>% 
  finalize_workflow(best_results) %>% 
  last_fit(split = concrete_split)

collect_metrics(boosting_test_results)

boosting_test_results %>% 
  collect_predictions() %>% 
  ggplot(aes(x = compressive_strength, y = .pred)) +
  geom_abline(lty = 2) +
  geom_point(alpha = 0.5) +
  coord_obs_pred()
```

