---
title: "Chapter 9: Judging Model Effectiveness"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(123)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)
  
lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)
```


Empirical validation means using data that were not used to create the model as the substrate to measure effectiveness. 

It's important to pick the right metric for model evaluation. For example, RMSE measures accuracy and R-squared measures correlation. 

# Performance Metrics and Inference 

An inferential model is used to understand relationships. 

Accessing how well the model fits the data is often left out of inferential analysis. 

# Regression Metrics

```{r}
ames_test_res <- predict(lm_fit, new_data = ames_test %>% select(-Sale_Price))
ames_test_res
```

Let's match the predicted values with their corresponding observed outcome values:

```{r}
ames_test_res <- bind_cols(ames_test_res, ames_test %>% select(Sale_Price))
ames_test_res
```

Let's plot the data:

```{r}
ggplot(ames_test_res, aes(x = Sale_Price, y = .pred)) +
  geom_abline(lty = 2) +
  geom_point(alpha = 0.5) +
  coord_obs_pred() +
  theme_minimal()
```

Let's compute the root mean squared error for this model:

```{r}
rmse(ames_test_res, truth = Sale_Price, estimate = .pred)
```

We can also compute multiple metrics at once:

```{r}
ames_metrics <- metric_set(rmse, rsq, mae)
ames_metrics(ames_test_res, truth = Sale_Price, estimate = .pred)
```

# Binary Classification Metrics

```{r}
data("two_class_example")
tibble(two_class_example)
```

```{r}
# confusion matrix:
conf_mat(two_class_example, truth = truth, estimate = predicted)

# accuracy
accuracy(two_class_example, truth, predicted)

# matthews correlation coefficient:
mcc(two_class_example, truth, predicted)

# F1 metric
f_meas(two_class_example, truth, predicted)

# combining these three metrics
classification_metrics <- metric_set(accuracy, mcc, f_meas)
classification_metrics(two_class_example, truth = truth, estimate = predicted)
```

For binary classification data sets, functions have a standard argument called `event_level`. The default is that the **first** level of the outcome factor is the event of interest. 

The receiver operating characteristic (ROC) curve computes the sensitivity and specificity over a continuum of different event thresholds. The predicted class column is not used. 

```{r}
two_class_curve <- roc_curve(two_class_example, truth, Class1)
two_class_curve

autoplot(two_class_curve)
```

If the curve was close to the line, the model's predictions would be no better than random guessing. Since the curve is up in the top, we see that our model preforms well at different thresholds. 

# Multi-class Classification Metrics

```{r}
data(hpc_cv)
tibble(hpc_cv)
```

As before, there are factors for the observed and predicted outcomes along with four other columns of predicted probabilities for each class. The functions for metrics that use the discrete class predictions are identical to their binary counterparts:

```{r}
accuracy(hpc_cv, obs, pred)

mcc(hpc_cv, obs, pred)
```

There are metrics such as sensitivity, that measures the true positive rate, which are specific to two classes. But they can be extended to multiple classes. We can do this a couple different ways:

* macro-averaging computes a set of one-versus-all metrics using the standard two-class statistics. These are averaged. 

* Macro-weighted averaging does the same but the average is weighted by the number of samples in each class

* Micro-averaging computes the contributes for each class, aggregates them, then computes a single metric from the aggregates.

```{r}
sensitivity(hpc_cv, obs, pred, estimator = "macro")

sensitivity(hpc_cv, obs, pred, estimator = "macro_weighted")

sensitivity(hpc_cv, obs, pred, estimator = "micro")
```

There are also some metrics for probability estimates. There is a multi-class technique for ROC curves:

```{r}
roc_auc(hpc_cv, obs, VF, F, M, L)

roc_auc(hpc_cv, obs, VF, F, M, L, estimator = "macro_weighted")
```

We can also pass a grouped data frame to get the metric function for each group:

```{r}
hpc_cv %>% 
  group_by(Resample) %>% 
  accuracy(obs, pred)

hpc_cv %>% 
  group_by(Resample) %>% 
  roc_curve(obs, VF, F, M, L) %>% 
  autoplot()
```

